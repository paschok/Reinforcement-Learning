{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning is an off-policy value-based method that uses a Temporal Difference Learning approach to train its action-value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All neeeded imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym #OPenAI - Taxi Environment\n",
    "import random # to generate random numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Tax-Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Q-table and initializing it\n",
    "\n",
    "Parameters *action_size* and *state_size* give info about how many rows (**states**) and columns (**actions**) we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "action_size, state_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the Qtable with zeros at the start of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qtable = np.zeros((state_size, action_size))\n",
    "Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 1000\n",
    "total_test_episodes = 100\n",
    "# max steps per episode\n",
    "max_steps = 99\n",
    "\n",
    "learning_rate = 0.7\n",
    "# discounting rate\n",
    "gamma = 0.6 \n",
    "\n",
    "# Exploration / Exploitation trade-off\n",
    "# exploration rate\n",
    "epsilon = 1.0\n",
    "# exploration probability at start\n",
    "epsilon_max = 1.0\n",
    "# minimum exploration probability \n",
    "epsilon_min = 0.01\n",
    "# exponential decay rate for exploration prob\n",
    "decay_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0,1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(Qtable[state,:])\n",
    "        \n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample() # returns the index of the action with the highest Q-value for that state\n",
    "        \n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        Qtable[state, action] = Qtable[state, action] + learning_rate * (reward + gamma * \n",
    "                                    np.max(Qtable[new_state, :]) - Qtable[state, action])\n",
    "                \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "    \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = epsilon_min + (epsilon_max - epsilon_min)*np.exp(-decay_rate*episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Taxi agent on Qtable\n",
    "\n",
    "After 50 000 episodes, Q-table can be used as a \"cheatsheet\" to play Taxi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE  0\n",
      "Score 9\n",
      "EPISODE  1\n",
      "Score 10\n",
      "EPISODE  2\n",
      "Score 7\n",
      "EPISODE  3\n",
      "Score 6\n",
      "EPISODE  4\n",
      "Score 11\n",
      "EPISODE  5\n",
      "Score 6\n",
      "EPISODE  6\n",
      "Score 3\n",
      "EPISODE  7\n",
      "Score 8\n",
      "EPISODE  8\n",
      "Score 11\n",
      "EPISODE  9\n",
      "Score 7\n",
      "EPISODE  10\n",
      "Score 5\n",
      "EPISODE  11\n",
      "Score 10\n",
      "EPISODE  12\n",
      "Score 5\n",
      "EPISODE  13\n",
      "Score 11\n",
      "EPISODE  14\n",
      "Score 5\n",
      "EPISODE  15\n",
      "Score 5\n",
      "EPISODE  16\n",
      "Score 9\n",
      "EPISODE  17\n",
      "Score 6\n",
      "EPISODE  18\n",
      "Score 9\n",
      "EPISODE  19\n",
      "Score 8\n",
      "EPISODE  20\n",
      "Score 9\n",
      "EPISODE  21\n",
      "Score 7\n",
      "EPISODE  22\n",
      "EPISODE  23\n",
      "Score 7\n",
      "EPISODE  24\n",
      "Score 9\n",
      "EPISODE  25\n",
      "Score 9\n",
      "EPISODE  26\n",
      "Score 8\n",
      "EPISODE  27\n",
      "Score 8\n",
      "EPISODE  28\n",
      "Score 9\n",
      "EPISODE  29\n",
      "Score 8\n",
      "EPISODE  30\n",
      "Score 8\n",
      "EPISODE  31\n",
      "EPISODE  32\n",
      "Score 7\n",
      "EPISODE  33\n",
      "Score 7\n",
      "EPISODE  34\n",
      "Score 7\n",
      "EPISODE  35\n",
      "Score 8\n",
      "EPISODE  36\n",
      "Score 10\n",
      "EPISODE  37\n",
      "Score 9\n",
      "EPISODE  38\n",
      "Score 8\n",
      "EPISODE  39\n",
      "Score 10\n",
      "EPISODE  40\n",
      "EPISODE  41\n",
      "Score 10\n",
      "EPISODE  42\n",
      "Score 4\n",
      "EPISODE  43\n",
      "Score 11\n",
      "EPISODE  44\n",
      "Score 8\n",
      "EPISODE  45\n",
      "Score 11\n",
      "EPISODE  46\n",
      "EPISODE  47\n",
      "Score 15\n",
      "EPISODE  48\n",
      "Score 10\n",
      "EPISODE  49\n",
      "Score 11\n",
      "EPISODE  50\n",
      "Score 9\n",
      "EPISODE  51\n",
      "Score 9\n",
      "EPISODE  52\n",
      "Score 8\n",
      "EPISODE  53\n",
      "Score 5\n",
      "EPISODE  54\n",
      "Score 6\n",
      "EPISODE  55\n",
      "Score 7\n",
      "EPISODE  56\n",
      "Score 11\n",
      "EPISODE  57\n",
      "Score 11\n",
      "EPISODE  58\n",
      "Score 10\n",
      "EPISODE  59\n",
      "Score 4\n",
      "EPISODE  60\n",
      "Score 5\n",
      "EPISODE  61\n",
      "Score 10\n",
      "EPISODE  62\n",
      "Score 9\n",
      "EPISODE  63\n",
      "Score 8\n",
      "EPISODE  64\n",
      "Score 9\n",
      "EPISODE  65\n",
      "Score 11\n",
      "EPISODE  66\n",
      "Score 7\n",
      "EPISODE  67\n",
      "Score 8\n",
      "EPISODE  68\n",
      "Score 9\n",
      "EPISODE  69\n",
      "Score 8\n",
      "EPISODE  70\n",
      "Score 8\n",
      "EPISODE  71\n",
      "EPISODE  72\n",
      "Score 11\n",
      "EPISODE  73\n",
      "Score 10\n",
      "EPISODE  74\n",
      "EPISODE  75\n",
      "Score 12\n",
      "EPISODE  76\n",
      "Score 9\n",
      "EPISODE  77\n",
      "Score 6\n",
      "EPISODE  78\n",
      "Score 6\n",
      "EPISODE  79\n",
      "Score 10\n",
      "EPISODE  80\n",
      "Score 4\n",
      "EPISODE  81\n",
      "Score 5\n",
      "EPISODE  82\n",
      "Score 12\n",
      "EPISODE  83\n",
      "Score 6\n",
      "EPISODE  84\n",
      "Score 8\n",
      "EPISODE  85\n",
      "Score 8\n",
      "EPISODE  86\n",
      "Score 6\n",
      "EPISODE  87\n",
      "Score 9\n",
      "EPISODE  88\n",
      "EPISODE  89\n",
      "Score 8\n",
      "EPISODE  90\n",
      "Score 12\n",
      "EPISODE  91\n",
      "Score 10\n",
      "EPISODE  92\n",
      "Score 5\n",
      "EPISODE  93\n",
      "Score 12\n",
      "EPISODE  94\n",
      "Score 4\n",
      "EPISODE  95\n",
      "Score 8\n",
      "EPISODE  96\n",
      "Score 13\n",
      "EPISODE  97\n",
      "Score 9\n",
      "EPISODE  98\n",
      "Score 9\n",
      "EPISODE  99\n",
      "Score 13\n",
      "Score over time: 7.76\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "rewards = []\n",
    "\n",
    "for episode in range(total_test_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    #print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # UNCOMMENT IT IF YOU WANT TO SEE OUR AGENT PLAYING\n",
    "        # env.render()\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(Qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            print (\"Score\", total_rewards)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
